{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Truth Detection through ML.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qOpkHOWxrGMJ",
        "VA9MYQdzYvUG",
        "oKVgcwPOOMGH",
        "1erF2TfVtvlF",
        "DHOMjP2wuBhj",
        "iBHClJfzrR--",
        "QquXKikTrfrS",
        "TGfF3Qkz3OUj",
        "57y4UyisIk2U",
        "iMqMNrVur4k2",
        "LPHag5mK2RN7",
        "QjFn2-0O1Rve",
        "U2vnFA7YU53Q"
      ],
      "authorship_tag": "ABX9TyPF52bDPdeE1HFAGM0cI8Qo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ATX24/truthDetection/blob/main/Truth_Detection_through_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOpkHOWxrGMJ"
      },
      "source": [
        "#Truth Detection through Machine Learning - Dhilan Shah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA9MYQdzYvUG"
      },
      "source": [
        "##Define Preliminary Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnXhs7RpYzOg"
      },
      "source": [
        "def tokenize(text):\n",
        "    clean_tokens = []\n",
        "    for token in text_to_nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
        "            clean_tokens.append(token.lemma_)\n",
        "    return clean_tokens\n",
        "\n",
        "def news(rating):\n",
        "  if rating == 0:\n",
        "    return 'This is real news'\n",
        "  else:\n",
        "    return 'This is fake news'\n",
        "\n",
        "def tokenize_vecs(text):\n",
        "    clean_tokens = []\n",
        "    for token in text_to_nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): \n",
        "          # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
        "            clean_tokens.append(token)\n",
        "    return clean_tokens\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKVgcwPOOMGH"
      },
      "source": [
        "##Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1erF2TfVtvlF"
      },
      "source": [
        "###Import NLP Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y_BayZhtt8l"
      },
      "source": [
        "import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv). \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import wordcloud\n",
        "import os # Good for navigating computer files \n",
        "import sys\n",
        "pd.options.mode.chained_assignment = None #suppress warnings\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "!python -m spacy download en_core_web_md\n",
        "import en_core_web_md\n",
        "text_to_nlp = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHOMjP2wuBhj"
      },
      "source": [
        "###Import Machine Learning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDnui8VJOmDw"
      },
      "source": [
        "from google.colab.output import eval_js\n",
        "\n",
        "import gdown\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "\n",
        "\n",
        "import cv2\n",
        "from google.colab import files\n",
        "import requests, io, zipfile\n",
        "!pip install patool\n",
        "import patoolib\n",
        "import os.path\n",
        "from os import path\n",
        "print(\"Downloaded Data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBHClJfzrR--"
      },
      "source": [
        "##Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QrUpokgCxxy"
      },
      "source": [
        "Data is uploaded from computer files (See \"Datasets Used\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDFAUrz8rU7a"
      },
      "source": [
        "#Only need to run once\n",
        "from google.colab import files\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg9B68w1o19t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23b9bd2-9439-4491-96d0-db22f20c5278"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdHyMjEjrW6-"
      },
      "source": [
        "uploaded = files.upload()\n",
        "dfTrue = pd.read_csv(io.BytesIO(uploaded['True.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InUhMLF8rZ9T"
      },
      "source": [
        "uploaded = files.upload()\n",
        "dfFake = pd.read_csv(io.BytesIO(uploaded['Fake.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIf09hW_rc5A"
      },
      "source": [
        "real = dfTrue['text']\n",
        "fake = dfFake['text']\n",
        "\n",
        "X_text = []\n",
        "y = [] #0 is true - 1 is false\n",
        "tst = 0\n",
        "tst2 = 0\n",
        "for var in range(1150):\n",
        "  ran = random.choice([0, 1])\n",
        "  if ran == 0:\n",
        "    X_text.append(real[var])\n",
        "    y.append(0)\n",
        "    tst = tst + 1\n",
        "  if ran == 1:\n",
        "    X_text.append(fake[var])\n",
        "    y.append(1)\n",
        "    tst2 = tst2 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QquXKikTrfrS"
      },
      "source": [
        "##Log model (Using Basic CountVectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCi6xt_YsbS4"
      },
      "source": [
        "def countveclog(X_text, y, example_review):\n",
        "  bow_transformer = CountVectorizer(analyzer=tokenize, max_features=1100).fit(X_text)\n",
        "  x = bow_transformer.transform(X_text)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=101)\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  prediction = model.predict(bow_transformer.transform([example_review]))\n",
        "  return news(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGfF3Qkz3OUj"
      },
      "source": [
        "##Word2vec (Used for the rest of the models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-NLsT5c3xd0"
      },
      "source": [
        "X_word2vec = []\n",
        "for text in X_text:\n",
        "  article = tokenize_vecs(text) # returns cleaned list of spacy tokens\n",
        "  rev = [0]*300\n",
        "\n",
        "  for word in article:\n",
        "    rev += word.vector  \n",
        "    \n",
        "  average = rev/len(article)\n",
        "  X_word2vec.append(average)\n",
        "      \n",
        "X_word2vec = np.array(X_word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57y4UyisIk2U"
      },
      "source": [
        "##Different Models that Implement Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMqMNrVur4k2"
      },
      "source": [
        "###Log model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SkNZHuWylEf"
      },
      "source": [
        "def word2veclog( X_word2vec, y, example_review): \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=101)\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  X_word2vecOne = []\n",
        "  article = tokenize_vecs(example_review) # returns cleaned list of spacy tokens\n",
        "\n",
        "  rev = [0]*300\n",
        "\n",
        "  for word in article:\n",
        "    rev += word.vector  \n",
        "    \n",
        "  average = rev/len(article)\n",
        "  X_word2vecOne.append(average)\n",
        "\n",
        "    \n",
        "  X_word2vecOne = np.array(X_word2vecOne)\n",
        "\n",
        "  prediction = model.predict(X_word2vecOne)\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  acc = accuracy_score(y_train, y_pred)\n",
        "  return news(prediction), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFEUwHSfzsw1"
      },
      "source": [
        "###KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5HPbHli0kIZ"
      },
      "source": [
        "def KNeighbors(X_word2vec, y, example_review): \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=101)\n",
        "\n",
        "  model = KNeighborsClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  X_word2vecOne = []\n",
        "  article = tokenize_vecs(example_review) # returns cleaned list of spacy tokens\n",
        "\n",
        "  rev = [0]*300\n",
        "\n",
        "  for word in article:\n",
        "    rev += word.vector  \n",
        "    \n",
        "  average = rev/len(article)\n",
        "  X_word2vecOne.append(average)\n",
        "\n",
        "    \n",
        "  X_word2vecOne = np.array(X_word2vecOne)\n",
        "\n",
        "  prediction = model.predict(X_word2vecOne)\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  acc = accuracy_score(y_train, y_pred)\n",
        "\n",
        "  return news(prediction), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkqCphjzysN"
      },
      "source": [
        "###GaussianProcessClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRA9ah6c1ePu"
      },
      "source": [
        "def GP(X_word2vec, y, example_review): \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=101)\n",
        "\n",
        "  model = GaussianProcessClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  X_word2vecOne = []\n",
        "  article = tokenize_vecs(example_review) # returns cleaned list of spacy tokens\n",
        "\n",
        "  rev = [0]*300\n",
        "\n",
        "  for word in article:\n",
        "    rev += word.vector  \n",
        "    \n",
        "  average = rev/len(article)\n",
        "  X_word2vecOne.append(average)\n",
        "\n",
        "    \n",
        "  X_word2vecOne = np.array(X_word2vecOne)\n",
        "  prediction = model.predict(X_word2vecOne)\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  acc = accuracy_score(y_train, y_pred)\n",
        "  return news(prediction), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjlKvo5Tzz9n"
      },
      "source": [
        "###DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IToRkfR11s14"
      },
      "source": [
        "def DTree(X_word2vec, y, example_review): \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=101)\n",
        "\n",
        "  model = DecisionTreeClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  X_word2vecOne = []\n",
        "  article = tokenize_vecs(example_review) # returns cleaned list of spacy tokens\n",
        "\n",
        "  rev = [0]*300\n",
        "\n",
        "  for word in article:\n",
        "    rev += word.vector  \n",
        "    \n",
        "  average = rev/len(article)\n",
        "  X_word2vecOne.append(average)\n",
        "\n",
        "    \n",
        "  X_word2vecOne = np.array(X_word2vecOne)\n",
        "\n",
        "  prediction = model.predict(X_word2vecOne)\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  acc = accuracy_score(y_train, y_pred)\n",
        "  return news(prediction), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9OVEOxW16hg"
      },
      "source": [
        "###AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7RaT0V02IaU"
      },
      "source": [
        "def Ada(X_word2vec, y, example_review): \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=101)\n",
        "\n",
        "  model = AdaBoostClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  X_word2vecOne = []\n",
        "  article = tokenize_vecs(example_review) # returns cleaned list of spacy tokens\n",
        "\n",
        "  rev = [0]*300\n",
        "\n",
        "  for word in article:\n",
        "    rev += word.vector  \n",
        "    \n",
        "  average = rev/len(article)\n",
        "  X_word2vecOne.append(average)\n",
        "\n",
        "    \n",
        "  X_word2vecOne = np.array(X_word2vecOne)\n",
        "\n",
        "  prediction = model.predict(X_word2vecOne)\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  acc = accuracy_score(y_train, y_pred)\n",
        "  return news(prediction), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEmczduf-zXk"
      },
      "source": [
        "###MLProcesser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M2xm6SK-5r7"
      },
      "source": [
        "def MLP(X_word2vec, y, example_review): \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=101)\n",
        "\n",
        "  model = MLPClassifier(max_iter = 1000)\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  X_word2vecOne = []\n",
        "  article = tokenize_vecs(example_review) # returns cleaned list of spacy tokens\n",
        "\n",
        "  rev = [0]*300\n",
        "\n",
        "  for word in article:\n",
        "    rev += word.vector  \n",
        "    \n",
        "  average = rev/len(article)\n",
        "  X_word2vecOne.append(average)\n",
        "\n",
        "    \n",
        "  X_word2vecOne = np.array(X_word2vecOne)\n",
        "\n",
        "  prediction = model.predict(X_word2vecOne)\n",
        "\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  acc = accuracy_score(y_train, y_pred)\n",
        "  return news(prediction), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaNP754U_UnK"
      },
      "source": [
        "###RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnVfH_SO_a0E"
      },
      "source": [
        "def RFC(X_word2vec, y, example_review): \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=101)\n",
        "\n",
        "  model = RandomForestClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  X_word2vecOne = []\n",
        "  article = tokenize_vecs(example_review) # returns cleaned list of spacy tokens\n",
        "\n",
        "  rev = [0]*300\n",
        "\n",
        "  for word in article:\n",
        "    rev += word.vector  \n",
        "    \n",
        "  average = rev/len(article)\n",
        "  X_word2vecOne.append(average)\n",
        "\n",
        "    \n",
        "  X_word2vecOne = np.array(X_word2vecOne)\n",
        "\n",
        "  prediction = model.predict(X_word2vecOne)\n",
        "\n",
        "  y_pred = model.predict(X_train)\n",
        "  acc = accuracy_score(y_train, y_pred)\n",
        "  return news(prediction), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPHag5mK2RN7"
      },
      "source": [
        "##Test stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjFn2-0O1Rve"
      },
      "source": [
        "###Test from text using a multi model test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-LUhfN82vRD"
      },
      "source": [
        "example_review = X_text[4]\n",
        "y[4] #Remember 1 is fake news and 0 is real news"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7RcAcPoLZIm"
      },
      "source": [
        "example_review = input('Test: ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XdKDa2Q5BFF"
      },
      "source": [
        "#Uses a function that runs each model for the given data\n",
        "\n",
        "\n",
        "def multiModelTest (X_word2vec, y, example_review):\n",
        "  dec1 = word2veclog( X_word2vec, y, example_review)\n",
        "  dec2 = KNeighbors(X_word2vec, y, example_review)\n",
        "  dec3 = GP(X_word2vec, y, example_review)\n",
        "  dec4 = DTree(X_word2vec, y, example_review)\n",
        "  dec5 = Ada(X_word2vec, y, example_review)\n",
        "  dec6 = MLP(X_word2vec, y, example_review)\n",
        "  dec7 = RFC(X_word2vec, y, example_review)\n",
        "\n",
        "  decisions = [dec1, dec2, dec3, dec4, dec5, dec5, dec7]\n",
        "  print(decisions)\n",
        "  fn = 0\n",
        "  rn = 0\n",
        "  for var in decisions:\n",
        "    var = var[0]\n",
        "    if var == ('This is fake news'):\n",
        "      fn = fn + 1\n",
        "    else:\n",
        "      rn = rn + 1\n",
        "\n",
        "  if rn > fn:\n",
        "    return 'This is real news'\n",
        "  else:\n",
        "    return 'This is fake news'\n",
        "\n",
        "\n",
        "def finalTest(X_word2vec, y, example_review):\n",
        "  decA = multiModelTest(X_word2vec, y, example_review)\n",
        "  decB = multiModelTest(X_word2vec, y, example_review)\n",
        "  decC = multiModelTest(X_word2vec, y, example_review)\n",
        "\n",
        "  deltaDecisions = [decA, decB, decC]\n",
        "  fn1 = 0\n",
        "  rn1 = 0\n",
        "  for var in deltaDecisions:\n",
        "    if var == ('This is fake news'):\n",
        "      fn1 = fn1 + 1\n",
        "    else:\n",
        "      rn1 = rn1 + 1\n",
        "  \n",
        "  print('trial 1: ' + decA)\n",
        "  print('trial 2: ' + decB)\n",
        "  print('trial 3: ' + decC)\n",
        "\n",
        "  if rn1 > fn1:\n",
        "    return 'This is real news'\n",
        "  else:\n",
        "    return 'This is fake news' \n",
        "\n",
        "\n",
        "finalTest(X_word2vec, y, example_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2vnFA7YU53Q"
      },
      "source": [
        "#Datasets Used: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUVPho48VJvL"
      },
      "source": [
        "https://www.uvic.ca/ecs/ece/isot/datasets/fake-news"
      ]
    }
  ]
}